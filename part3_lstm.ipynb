{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             41188 non-null  int64  \n",
      " 1   job             41188 non-null  object \n",
      " 2   marital         41188 non-null  object \n",
      " 3   education       41188 non-null  object \n",
      " 4   default         41188 non-null  object \n",
      " 5   housing         41188 non-null  object \n",
      " 6   loan            41188 non-null  object \n",
      " 7   contact         41188 non-null  object \n",
      " 8   month           41188 non-null  object \n",
      " 9   day_of_week     41188 non-null  object \n",
      " 10  duration        41188 non-null  int64  \n",
      " 11  campaign        41188 non-null  int64  \n",
      " 12  pdays           41188 non-null  int64  \n",
      " 13  previous        41188 non-null  int64  \n",
      " 14  poutcome        41188 non-null  object \n",
      " 15  emp.var.rate    41188 non-null  float64\n",
      " 16  cons.price.idx  41188 non-null  float64\n",
      " 17  cons.conf.idx   41188 non-null  float64\n",
      " 18  euribor3m       41188 non-null  float64\n",
      " 19  nr.employed     41188 non-null  float64\n",
      " 20  y               41188 non-null  object \n",
      "dtypes: float64(5), int64(5), object(11)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "bank_data = pd.read_csv('data/bank-full.csv', sep=';')\n",
    "bank_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing the dataset to only include features I want in my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = bank_data.loc[:,['age','marital','default','housing','loan','cons.price.idx','cons.conf.idx','y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the several feature columns and the prediction column y to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown default is considered 'no'\n",
    "bank_data['default'] = bank_data['default'].map({'no':0,'yes':1,'unknown':0})\n",
    "\n",
    "# Unknown housing is considered 'no'\n",
    "bank_data['housing'] = bank_data['housing'].map({'no':0,'yes':1,'unknown':0})\n",
    "\n",
    "# Unknown loan is considered 'no'\n",
    "bank_data['loan'] = bank_data['loan'].map({'no':0,'yes':1,'unknown':0})\n",
    "\n",
    "# Unknown marital is considered single\n",
    "bank_data['marital'] = bank_data['marital'].map({'divorced':0,'single':1,'married':2,'unknown':1})\n",
    "\n",
    "bank_data['y'] = bank_data['y'].map({'no':0,'yes':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             41188 non-null  int64  \n",
      " 1   marital         41188 non-null  int64  \n",
      " 2   default         41188 non-null  int64  \n",
      " 3   housing         41188 non-null  int64  \n",
      " 4   loan            41188 non-null  int64  \n",
      " 5   cons.price.idx  41188 non-null  float64\n",
      " 6   cons.conf.idx   41188 non-null  float64\n",
      " 7   y               41188 non-null  int64  \n",
      "dtypes: float64(2), int64(6)\n",
      "memory usage: 2.5 MB\n"
     ]
    }
   ],
   "source": [
    "bank_data = bank_data.sample(frac=1).reset_index(drop=True)\n",
    "bank_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bank_data.drop('y', axis=1)\n",
    "y = bank_data['y']\n",
    "\n",
    "MinMaxScaler = preprocessing.MinMaxScaler()\n",
    "X = MinMaxScaler.fit_transform(x)\n",
    "X = pd.DataFrame(X, columns=['age', 'marital', 'default', 'housing', 'loan', 'cons.price.idx', 'cons.conf.idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(LSTM(100))\n",
    "# model.add(LSTM(49))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8834 - loss: 1.1926\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8837 - loss: 1.1928\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8839 - loss: 1.1926\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8839 - loss: 1.1925\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8839 - loss: 1.1926\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.8839 - loss: 1.1923\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8844 - loss: 1.1922\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8844 - loss: 1.1919\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8844 - loss: 1.1918\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8848 - loss: 1.1916\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8845 - loss: 1.1940\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8845 - loss: 1.1938\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8847 - loss: 1.1937\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8847 - loss: 1.1936\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8847 - loss: 1.1935\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8847 - loss: 1.1934\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8850 - loss: 1.1933\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8847 - loss: 1.1933\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8850 - loss: 1.1931\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8851 - loss: 1.1930\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8868 - loss: 1.1701\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8869 - loss: 1.1699\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8869 - loss: 1.1698\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8869 - loss: 1.1698\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8869 - loss: 1.1696\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8869 - loss: 1.1696\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8869 - loss: 1.1695\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8870 - loss: 1.1694\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8872 - loss: 1.1693\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8870 - loss: 1.1693\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8866 - loss: 1.1767\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8866 - loss: 1.1767\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8866 - loss: 1.1766\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8866 - loss: 1.1765\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8866 - loss: 1.1764\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8866 - loss: 1.1764\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8866 - loss: 1.1763\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8866 - loss: 1.1763\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8866 - loss: 1.1761\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8869 - loss: 1.1761\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8857 - loss: 1.1892\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8859 - loss: 1.1892\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8856 - loss: 1.1889\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8856 - loss: 1.1887\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8857 - loss: 1.1886\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8859 - loss: 1.1884\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8859 - loss: 1.1883\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8859 - loss: 1.1882\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8861 - loss: 1.1881\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8861 - loss: 1.1880\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8875 - loss: 1.1700\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8875 - loss: 1.1698\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8875 - loss: 1.1697\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8877 - loss: 1.1696\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8877 - loss: 1.1694\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8877 - loss: 1.1694\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8877 - loss: 1.1693\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8877 - loss: 1.1692\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8877 - loss: 1.1691\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8877 - loss: 1.1690\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8875 - loss: 1.1709\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8876 - loss: 1.1708\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8876 - loss: 1.1707\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8876 - loss: 1.1706\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8876 - loss: 1.1706\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8876 - loss: 1.1705\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8876 - loss: 1.1704\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8876 - loss: 1.1703\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8876 - loss: 1.1703\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8876 - loss: 1.1702\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8874 - loss: 1.1696\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8874 - loss: 1.1695\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8874 - loss: 1.1695\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8874 - loss: 1.1694\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8874 - loss: 1.1693\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8874 - loss: 1.1693\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8874 - loss: 1.1693\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8874 - loss: 1.1691\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8874 - loss: 1.1691\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8874 - loss: 1.1690\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8875 - loss: 1.1704\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8875 - loss: 1.1703\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8875 - loss: 1.1703\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8875 - loss: 1.1702\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8876 - loss: 1.1702\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8876 - loss: 1.1701\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8876 - loss: 1.1700\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8876 - loss: 1.1699\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8876 - loss: 1.1699\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8876 - loss: 1.1698\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8863 - loss: 1.1868\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8863 - loss: 1.1865\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8863 - loss: 1.1864\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8863 - loss: 1.1863\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8863 - loss: 1.1861\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8863 - loss: 1.1860\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8863 - loss: 1.1860\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8863 - loss: 1.1859\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8863 - loss: 1.1858\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8863 - loss: 1.1858\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "classifier_performance = []\n",
    "\n",
    "for i, (train_indices, test_indices) in enumerate(kf.split(X)):\n",
    "    X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "    X_test, y_test = X.iloc[test_indices], y.iloc[test_indices]\n",
    "\n",
    "    model.fit(X_train.to_numpy(), y_train.to_numpy(), batch_size=len(X_train), epochs=10)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred[y_pred <= 0] = 0\n",
    "    y_pred[y_pred > 0] = 1\n",
    "    y_pred\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    # TP = cm[0][0]\n",
    "    # FN = cm[1][0]\n",
    "    # TN = cm[1][1]\n",
    "    # FP = cm[0][1]\n",
    "    P = TP + FN\n",
    "    N = TN + FP\n",
    "    TPR = TP/P\n",
    "    TNR = TN/N\n",
    "    FPR = FP/N\n",
    "    FNR = FN/P\n",
    "    r = TP/P\n",
    "    p = TP/(TP+FP)\n",
    "    F1 = 2*(p*r)/(p+r)\n",
    "    Acc = (TP+TN)/(P+N)\n",
    "    Err = (FP+FN)/(P+N)\n",
    "    BACC = (TPR + TNR)/2\n",
    "    TSS = TP/(TP+FN) - FP/(FP+TN)\n",
    "    HSS = 2*(TP*TN - FP*FN) / ((TP+FN)*(FN+TN) + (TP+FP)*(FP+TN))\n",
    "    classifier_performance.append([i, TP, TN, FP, FN, P, N, TPR, TNR, FPR, FNR, r, p, F1, Acc, Err, BACC, TSS, HSS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>P</th>\n",
       "      <th>N</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TNR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>FNR</th>\n",
       "      <th>r</th>\n",
       "      <th>p</th>\n",
       "      <th>F1</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Err</th>\n",
       "      <th>BACC</th>\n",
       "      <th>TSS</th>\n",
       "      <th>HSS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>2059</td>\n",
       "      <td>1620</td>\n",
       "      <td>233</td>\n",
       "      <td>440</td>\n",
       "      <td>3679</td>\n",
       "      <td>0.470455</td>\n",
       "      <td>0.559663</td>\n",
       "      <td>0.440337</td>\n",
       "      <td>0.529545</td>\n",
       "      <td>0.470455</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.182620</td>\n",
       "      <td>0.550134</td>\n",
       "      <td>0.449866</td>\n",
       "      <td>0.515059</td>\n",
       "      <td>0.030117</td>\n",
       "      <td>0.012614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>2086</td>\n",
       "      <td>1600</td>\n",
       "      <td>228</td>\n",
       "      <td>433</td>\n",
       "      <td>3686</td>\n",
       "      <td>0.473441</td>\n",
       "      <td>0.565925</td>\n",
       "      <td>0.434075</td>\n",
       "      <td>0.526559</td>\n",
       "      <td>0.473441</td>\n",
       "      <td>0.113573</td>\n",
       "      <td>0.183199</td>\n",
       "      <td>0.556203</td>\n",
       "      <td>0.443797</td>\n",
       "      <td>0.519683</td>\n",
       "      <td>0.039366</td>\n",
       "      <td>0.016415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>219</td>\n",
       "      <td>2055</td>\n",
       "      <td>1567</td>\n",
       "      <td>278</td>\n",
       "      <td>497</td>\n",
       "      <td>3622</td>\n",
       "      <td>0.440644</td>\n",
       "      <td>0.567366</td>\n",
       "      <td>0.432634</td>\n",
       "      <td>0.559356</td>\n",
       "      <td>0.440644</td>\n",
       "      <td>0.122620</td>\n",
       "      <td>0.191853</td>\n",
       "      <td>0.552076</td>\n",
       "      <td>0.447924</td>\n",
       "      <td>0.504005</td>\n",
       "      <td>0.008010</td>\n",
       "      <td>0.003780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>215</td>\n",
       "      <td>2095</td>\n",
       "      <td>1546</td>\n",
       "      <td>263</td>\n",
       "      <td>478</td>\n",
       "      <td>3641</td>\n",
       "      <td>0.449791</td>\n",
       "      <td>0.575391</td>\n",
       "      <td>0.424609</td>\n",
       "      <td>0.550209</td>\n",
       "      <td>0.449791</td>\n",
       "      <td>0.122090</td>\n",
       "      <td>0.192050</td>\n",
       "      <td>0.560816</td>\n",
       "      <td>0.439184</td>\n",
       "      <td>0.512591</td>\n",
       "      <td>0.025182</td>\n",
       "      <td>0.011627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>209</td>\n",
       "      <td>2126</td>\n",
       "      <td>1551</td>\n",
       "      <td>233</td>\n",
       "      <td>442</td>\n",
       "      <td>3677</td>\n",
       "      <td>0.472851</td>\n",
       "      <td>0.578189</td>\n",
       "      <td>0.421811</td>\n",
       "      <td>0.527149</td>\n",
       "      <td>0.472851</td>\n",
       "      <td>0.118750</td>\n",
       "      <td>0.189827</td>\n",
       "      <td>0.566885</td>\n",
       "      <td>0.433115</td>\n",
       "      <td>0.525520</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>0.022078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>214</td>\n",
       "      <td>2079</td>\n",
       "      <td>1552</td>\n",
       "      <td>274</td>\n",
       "      <td>488</td>\n",
       "      <td>3631</td>\n",
       "      <td>0.438525</td>\n",
       "      <td>0.572570</td>\n",
       "      <td>0.427430</td>\n",
       "      <td>0.561475</td>\n",
       "      <td>0.438525</td>\n",
       "      <td>0.121178</td>\n",
       "      <td>0.189885</td>\n",
       "      <td>0.556689</td>\n",
       "      <td>0.443311</td>\n",
       "      <td>0.505547</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>213</td>\n",
       "      <td>2047</td>\n",
       "      <td>1589</td>\n",
       "      <td>270</td>\n",
       "      <td>483</td>\n",
       "      <td>3636</td>\n",
       "      <td>0.440994</td>\n",
       "      <td>0.562981</td>\n",
       "      <td>0.437019</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.440994</td>\n",
       "      <td>0.118202</td>\n",
       "      <td>0.186433</td>\n",
       "      <td>0.548677</td>\n",
       "      <td>0.451323</td>\n",
       "      <td>0.501988</td>\n",
       "      <td>0.003975</td>\n",
       "      <td>0.001820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>202</td>\n",
       "      <td>2050</td>\n",
       "      <td>1595</td>\n",
       "      <td>272</td>\n",
       "      <td>474</td>\n",
       "      <td>3645</td>\n",
       "      <td>0.426160</td>\n",
       "      <td>0.562414</td>\n",
       "      <td>0.437586</td>\n",
       "      <td>0.573840</td>\n",
       "      <td>0.426160</td>\n",
       "      <td>0.112410</td>\n",
       "      <td>0.177895</td>\n",
       "      <td>0.546735</td>\n",
       "      <td>0.453265</td>\n",
       "      <td>0.494287</td>\n",
       "      <td>-0.011425</td>\n",
       "      <td>-0.005160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>206</td>\n",
       "      <td>2110</td>\n",
       "      <td>1533</td>\n",
       "      <td>269</td>\n",
       "      <td>475</td>\n",
       "      <td>3643</td>\n",
       "      <td>0.433684</td>\n",
       "      <td>0.579193</td>\n",
       "      <td>0.420807</td>\n",
       "      <td>0.566316</td>\n",
       "      <td>0.433684</td>\n",
       "      <td>0.118459</td>\n",
       "      <td>0.186089</td>\n",
       "      <td>0.562409</td>\n",
       "      <td>0.437591</td>\n",
       "      <td>0.506439</td>\n",
       "      <td>0.012877</td>\n",
       "      <td>0.005970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>202</td>\n",
       "      <td>2088</td>\n",
       "      <td>1600</td>\n",
       "      <td>228</td>\n",
       "      <td>430</td>\n",
       "      <td>3688</td>\n",
       "      <td>0.469767</td>\n",
       "      <td>0.566161</td>\n",
       "      <td>0.433839</td>\n",
       "      <td>0.530233</td>\n",
       "      <td>0.469767</td>\n",
       "      <td>0.112098</td>\n",
       "      <td>0.181004</td>\n",
       "      <td>0.556095</td>\n",
       "      <td>0.443905</td>\n",
       "      <td>0.517964</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.014912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index   TP    TN    FP   FN    P     N       TPR       TNR       FPR  \\\n",
       "0      0  207  2059  1620  233  440  3679  0.470455  0.559663  0.440337   \n",
       "1      1  205  2086  1600  228  433  3686  0.473441  0.565925  0.434075   \n",
       "2      2  219  2055  1567  278  497  3622  0.440644  0.567366  0.432634   \n",
       "3      3  215  2095  1546  263  478  3641  0.449791  0.575391  0.424609   \n",
       "4      4  209  2126  1551  233  442  3677  0.472851  0.578189  0.421811   \n",
       "5      5  214  2079  1552  274  488  3631  0.438525  0.572570  0.427430   \n",
       "6      6  213  2047  1589  270  483  3636  0.440994  0.562981  0.437019   \n",
       "7      7  202  2050  1595  272  474  3645  0.426160  0.562414  0.437586   \n",
       "8      8  206  2110  1533  269  475  3643  0.433684  0.579193  0.420807   \n",
       "9      9  202  2088  1600  228  430  3688  0.469767  0.566161  0.433839   \n",
       "\n",
       "        FNR         r         p        F1       Acc       Err      BACC  \\\n",
       "0  0.529545  0.470455  0.113300  0.182620  0.550134  0.449866  0.515059   \n",
       "1  0.526559  0.473441  0.113573  0.183199  0.556203  0.443797  0.519683   \n",
       "2  0.559356  0.440644  0.122620  0.191853  0.552076  0.447924  0.504005   \n",
       "3  0.550209  0.449791  0.122090  0.192050  0.560816  0.439184  0.512591   \n",
       "4  0.527149  0.472851  0.118750  0.189827  0.566885  0.433115  0.525520   \n",
       "5  0.561475  0.438525  0.121178  0.189885  0.556689  0.443311  0.505547   \n",
       "6  0.559006  0.440994  0.118202  0.186433  0.548677  0.451323  0.501988   \n",
       "7  0.573840  0.426160  0.112410  0.177895  0.546735  0.453265  0.494287   \n",
       "8  0.566316  0.433684  0.118459  0.186089  0.562409  0.437591  0.506439   \n",
       "9  0.530233  0.469767  0.112098  0.181004  0.556095  0.443905  0.517964   \n",
       "\n",
       "        TSS       HSS  \n",
       "0  0.030117  0.012614  \n",
       "1  0.039366  0.016415  \n",
       "2  0.008010  0.003780  \n",
       "3  0.025182  0.011627  \n",
       "4  0.051039  0.022078  \n",
       "5  0.011094  0.005200  \n",
       "6  0.003975  0.001820  \n",
       "7 -0.011425 -0.005160  \n",
       "8  0.012877  0.005970  \n",
       "9  0.035928  0.014912  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['Index', 'TP', 'TN', 'FP', 'FN', 'P', 'N', 'TPR', 'TNR', 'FPR', 'FNR', 'r', 'p', 'F1', 'Acc', 'Err', 'BACC', 'TSS', 'HSS']\n",
    "class_perf_df = pd.DataFrame(classifier_performance, columns=columns)\n",
    "class_perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for Average Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Method        LSTM\n",
       "TP           209.2\n",
       "TN          2079.5\n",
       "FP          1575.3\n",
       "FN           254.8\n",
       "P            464.0\n",
       "N           3654.8\n",
       "TPR       0.451631\n",
       "TNR       0.568985\n",
       "FPR       0.431015\n",
       "FNR       0.548369\n",
       "r         0.451631\n",
       "p         0.117268\n",
       "F1        0.186085\n",
       "Acc       0.555672\n",
       "Err       0.444328\n",
       "BACC      0.510308\n",
       "TSS       0.020616\n",
       "HSS       0.008926\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_perf_df = class_perf_df.fillna(0)\n",
    "\n",
    "average_row = class_perf_df.mean(axis=0)\n",
    "average_row = average_row.drop(average_row.index[0])\n",
    "df = pd.DataFrame(average_row).T\n",
    "method = pd.DataFrame([{'Method': 'LSTM'}])\n",
    "df.insert(0, 'Method', method)\n",
    "df.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN method has an average accuracy of 0.5557."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
